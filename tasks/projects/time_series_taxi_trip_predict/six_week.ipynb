{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import pickle\n",
    "from scipy import signal\n",
    "import scipy as sc\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные о количество поездок в 102 регионах, выбранных на 2 неделе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/all_regions.pkl\", \"rb\") as fid:\n",
    "    data = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/all_regions_pass_count.pkl\", \"rb\") as fid:\n",
    "    data1 = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21888L, 102L)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.transpose().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и идентификаторы оставшихся регионов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/regions_left.pkl\", \"rb\") as fid:\n",
    "    regions = np.array(pickle.load(fid)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем таблицу. По строкам у нас время, а по столбцам идентификатор ячейки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1075</th>\n",
       "      <th>1076</th>\n",
       "      <th>1077</th>\n",
       "      <th>1125</th>\n",
       "      <th>1126</th>\n",
       "      <th>1127</th>\n",
       "      <th>1128</th>\n",
       "      <th>1129</th>\n",
       "      <th>1130</th>\n",
       "      <th>1131</th>\n",
       "      <th>...</th>\n",
       "      <th>1630</th>\n",
       "      <th>1684</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1783</th>\n",
       "      <th>2068</th>\n",
       "      <th>2069</th>\n",
       "      <th>2118</th>\n",
       "      <th>2119</th>\n",
       "      <th>2168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:00:00</th>\n",
       "      <td>87.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>367.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:00:00</th>\n",
       "      <td>92.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 02:00:00</th>\n",
       "      <td>108.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 03:00:00</th>\n",
       "      <td>77.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 04:00:00</th>\n",
       "      <td>47.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      1075   1076  1077   1125   1126   1127   1128   1129  \\\n",
       "2014-01-01 00:00:00   87.0  146.0  70.0  113.0  367.0  645.0  589.0  799.0   \n",
       "2014-01-01 01:00:00   92.0  184.0  93.0  153.0  539.0  604.0  490.0  635.0   \n",
       "2014-01-01 02:00:00  108.0  165.0  55.0  151.0  443.0  571.0  465.0  499.0   \n",
       "2014-01-01 03:00:00   77.0  108.0  32.0  112.0  372.0  533.0  442.0  370.0   \n",
       "2014-01-01 04:00:00   47.0   79.0  22.0   77.0  213.0  383.0  296.0  319.0   \n",
       "\n",
       "                      1130   1131  ...   1630  1684  1733  1734  1783  2068  \\\n",
       "2014-01-01 00:00:00  948.0  321.0  ...    9.0   0.0   5.0  89.0  10.0  35.0   \n",
       "2014-01-01 01:00:00  667.0  225.0  ...   24.0   0.0   3.0  22.0   2.0   5.0   \n",
       "2014-01-01 02:00:00  455.0  124.0  ...   27.0   0.0   3.0  23.0   1.0   1.0   \n",
       "2014-01-01 03:00:00  307.0  101.0  ...   57.0   0.0   0.0   3.0   2.0   1.0   \n",
       "2014-01-01 04:00:00  261.0   87.0  ...   38.0   0.0   1.0   9.0   1.0   8.0   \n",
       "\n",
       "                     2069   2118  2119  2168  \n",
       "2014-01-01 00:00:00   9.0  106.0  22.0  71.0  \n",
       "2014-01-01 01:00:00   0.0   87.0   0.0  44.0  \n",
       "2014-01-01 02:00:00   0.0   39.0   0.0   1.0  \n",
       "2014-01-01 03:00:00   0.0    5.0   1.0   0.0  \n",
       "2014-01-01 04:00:00   0.0   29.0   1.0  18.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.date_range('2014-01-01 00:00:00', periods=data.shape[1], freq='H')\n",
    "df = pd.DataFrame(data.transpose(), index=p, columns=regions)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1075</th>\n",
       "      <th>1076</th>\n",
       "      <th>1077</th>\n",
       "      <th>1125</th>\n",
       "      <th>1126</th>\n",
       "      <th>1127</th>\n",
       "      <th>1128</th>\n",
       "      <th>1129</th>\n",
       "      <th>1130</th>\n",
       "      <th>1131</th>\n",
       "      <th>...</th>\n",
       "      <th>1630</th>\n",
       "      <th>1684</th>\n",
       "      <th>1733</th>\n",
       "      <th>1734</th>\n",
       "      <th>1783</th>\n",
       "      <th>2068</th>\n",
       "      <th>2069</th>\n",
       "      <th>2118</th>\n",
       "      <th>2119</th>\n",
       "      <th>2168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-01 00:00:00</th>\n",
       "      <td>184.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>591.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 01:00:00</th>\n",
       "      <td>167.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>1121.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 02:00:00</th>\n",
       "      <td>210.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>818.0</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>877.0</td>\n",
       "      <td>865.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 03:00:00</th>\n",
       "      <td>148.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01 04:00:00</th>\n",
       "      <td>104.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      1075   1076   1077   1125    1126    1127    1128  \\\n",
       "2014-01-01 00:00:00  184.0  237.0  126.0  180.0   718.0  1271.0  1085.0   \n",
       "2014-01-01 01:00:00  167.0  334.0  173.0  297.0  1112.0  1121.0  1003.0   \n",
       "2014-01-01 02:00:00  210.0  305.0   91.0  295.0   818.0  1053.0   877.0   \n",
       "2014-01-01 03:00:00  148.0  175.0   54.0  209.0   662.0   963.0   829.0   \n",
       "2014-01-01 04:00:00  104.0  158.0   44.0  154.0   403.0   754.0   537.0   \n",
       "\n",
       "                       1129    1130   1131  ...     1630  1684  1733   1734  \\\n",
       "2014-01-01 00:00:00  1458.0  1731.0  591.0  ...     22.0   0.0  10.0  177.0   \n",
       "2014-01-01 01:00:00  1184.0  1296.0  424.0  ...     41.0   0.0   4.0   27.0   \n",
       "2014-01-01 02:00:00   865.0   911.0  250.0  ...     42.0   0.0   4.0   40.0   \n",
       "2014-01-01 03:00:00   689.0   570.0  181.0  ...    104.0   0.0   0.0    4.0   \n",
       "2014-01-01 04:00:00   570.0   485.0  159.0  ...     65.0   0.0   1.0   13.0   \n",
       "\n",
       "                     1783  2068  2069   2118  2119   2168  \n",
       "2014-01-01 00:00:00  20.0  50.0  13.0  187.0  43.0  148.0  \n",
       "2014-01-01 01:00:00   2.0   6.0   0.0  132.0   0.0  101.0  \n",
       "2014-01-01 02:00:00   1.0   1.0   0.0   66.0   0.0    5.0  \n",
       "2014-01-01 03:00:00   3.0   1.0   0.0    7.0   1.0    0.0  \n",
       "2014-01-01 04:00:00   2.0   9.0   0.0   59.0   1.0   31.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pass_cnt = pd.DataFrame(data1.transpose(), index=p, columns=regions)\n",
    "df_pass_cnt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее привоодятся функции создающие синусы/косинусы и dummy-признаки дней недели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_fourier_regressors(data_f, num_end, season=168.0, f_ind='week_'):\n",
    "    str_var = ''\n",
    "    length = data_f.shape[0]\n",
    "    for i in range(1, num_end+1):\n",
    "        sin = \"s_\" + f_ind + str(i)\n",
    "        cos = \"c_\" + f_ind + str(i)\n",
    "        data_f[sin] = np.sin(2*np.pi*i*np.arange(1, length+1)/season)\n",
    "        data_f[cos] = np.cos(2*np.pi*i*np.arange(1, length+1)/season)\n",
    "        str_var = str_var + sin + ' + '\n",
    "        if i != num_end:\n",
    "            str_var = str_var + cos + ' + '\n",
    "        else:\n",
    "            str_var = str_var + cos\n",
    "    return str_var\n",
    "\n",
    "def make_dummy_weekday(data):\n",
    "    # Weekday dummy-variables\n",
    "    data['monday'] = [1 if date.weekday() == 0 else 0 for date in data.index]\n",
    "    data['tuesday'] = [1 if date.weekday() == 1 else 0 for date in data.index]\n",
    "    data['wednessday'] = [1 if date.weekday() == 2 else 0 for date in data.index]\n",
    "    data['thursday'] = [1 if date.weekday() == 3 else 0 for date in data.index]\n",
    "    data['friday'] = [1 if date.weekday() == 4 else 0 for date in data.index]\n",
    "    data['saturday'] = [1 if date.weekday() == 5 else 0 for date in data.index]\n",
    "    data['sunday'] = [1 if date.weekday() == 6 else 0 for date in data.index]\n",
    "    weekday_str = ' + tuesday + wednessday + thursday + friday + saturday + sunday'\n",
    "    return weekday_str\n",
    "\n",
    "def fourier_prediction(data, train_time_limit, degree=49):\n",
    "    data_c = pd.DataFrame(data.values, columns = ['val'], index = data.index)\n",
    "    str_reg = 'val ~ '\n",
    "    week_day_str, str_var = '', ''\n",
    "    str_var = make_fourier_regressors(data_c, degree)\n",
    "    week_day_str = make_dummy_weekday(data_c)\n",
    "    model = smf.ols(str_reg + str_var + week_day_str, data=data_c.loc[:train_time_limit])\n",
    "    fitted = model.fit(cov_type='HC1')\n",
    "    #regressor = ElasticNet(alpha=0.1, l1_ratio=0.6)\n",
    "    #regressor.fit(data_c.loc[:train_time_limit].drop(['val'], axis=1), data_c.loc[:train_time_limit].val)\n",
    "    return fitted.predict(data_c)\n",
    "    #return regressor.predict(data_c.drop(['val'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приводится функция, которая создает соответствующие выборки, обучает модели и делает предскаазания на соответствующий период времени. Используем теже признаки, что и выше, плюс скользящую сумму с окном размера 12. Параметры регрессионной модели подобраны одни и теже для всех регионов на основе качестве предсказания на майских данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлой неделе были использованы следующие признаки: \n",
    "1. дневные и часовые лаги\n",
    "2. скользящие суммы по 12 часов \n",
    "3. dummy-признаки дней недели\n",
    "4. синусы/косинусы, соответствующие недельной сезонности\n",
    "\n",
    "План на действий на данную неделю:\n",
    "1. Добавить синусы/косинусы, описывающие годовую сезонность.\n",
    "2. Добавить скользящих сумм.\n",
    "3. Добавить dummy-признаки праздников\n",
    "4. Добавить признак - температура\n",
    "5. Добавить признак - температура с учетом ветра\n",
    "6. Добавить признак - количество осадков за день\n",
    "7. Добавить признак - средняя скорость ветра\n",
    "8. Воспользоваться XGBoost регрессором, который позволяет также использовать l1 и l2 регуляризацию, но, на мой взгляд, также позволяет и учитывать некторые взаимосвязи между признаками.\n",
    "9. Сабмит решения, результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на исходное решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region(data, train_time_limit, test_time_limit, region, denom, pred_start='2016-05-01', pred_end='2016-05-31', degree=49,\n",
    "                       K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание для некоторых регионов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109867415613\n",
      "0.825472819945\n",
      "0.181854162141\n",
      "Wall time: 35.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168, denom)\n",
    "print err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем предсказания для мая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20.6100811262\n",
      "Wall time: 15min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region, denom)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка предсказания на май 2016 года. Она значительно уменьшилась по сравнению с прошлой неделей: было около 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем предсказания на июнь по данным до мая 2016 года и считаем ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.3926429294\n",
      "CPU times: user 19min 13s, sys: 2min 24s, total: 21min 37s\n",
      "Wall time: 11min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 715\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_june = 0\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region(df.loc[:'2016-06-30 23:00:00'], \\\n",
    "                                        '2016-05-31 23:00:00', '2016-06-30 17:00:00', \n",
    "                                        region, denom,\n",
    "                                        '2016-06-01', '2016-06-30')\n",
    "    Q_june += res\n",
    "    all_ids.append(ids)\n",
    "    all_preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем dummy-признаки праздничных дней при построении моделей для каждого региона и для каждого "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region1(data, train_time_limit, test_time_limit, region, denom, pred_start='2016-05-01', \\\n",
    "                        pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Year seasonality\n",
    "    make_fourier_regressors(new_data, 6, 8766.0, 'year_')\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.114892784153\n",
      "0.857273501547\n",
      "0.181507018872\n",
      "Wall time: 34.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region1(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region1(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region1(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168, denom)\n",
    "print err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество ухудшилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21.480671735\n",
      "Wall time: 17min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region1(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region, denom)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление годовой сезонности заметно ухудшило модель, откажемся от их использования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим dummy-признаки праздничных дней и дней рядом с праздничными днями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region2(data, train_time_limit, test_time_limit, region, denom, pred_start='2016-05-01', \\\n",
    "                        pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    \n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109665732675\n",
      "0.819150375985\n",
      "0.181595258414\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region2(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region2(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region2(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168, denom)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20.5514422138\n",
      "Wall time: 18min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region2(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region, denom)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование праздников дало некоторый прирост качества, поэтому оставим данные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем добавлять скользящие суммы за 6, 24, 168 часов (12 было в исходной модели)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region3(data, train_time_limit, test_time_limit, region, denom, pred_start='2016-05-01', \\\n",
    "                        pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    \n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108402477944\n",
      "0.722976334698\n",
      "0.176678274037\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region3(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region3(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232, denom)\n",
    "print err\n",
    "err, _, _ = count_error_region3(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168, denom)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "19.0094494726\n",
      "Wall time: 35min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region3(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region, denom)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество заметно улучшилось, оставим эти признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем использовать почасовые температурные показатели. Эти данные были получены с помощью парсинга сайта . Код приводится в конце ноутбука."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в рассмотрение температуру окружающей среды в качестве числового признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  21888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-3.9, -3.9, -4.4, -4.4, -4.4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"hourly_temp_data.pkl\", 'rb') as fid:\n",
    "    temperature = pickle.load(fid)\n",
    "print \"Data size: \", len(temperature)\n",
    "temperature = map(float, temperature)\n",
    "temperature[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region4(data, train_time_limit, test_time_limit, region, denom, temperature,\\\n",
    "                        pred_start='2016-05-01', pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    #*************\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    #********\n",
    "    # Temperature\n",
    "    new_data['temperature'] = temperature[:new_data.shape[0]]\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4, max_iter=1500)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108018391827\n",
      "0.721881773709\n",
      "0.176647576885\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region4(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075,\\\n",
    "                                denom, temperature)\n",
    "print err\n",
    "err, _, _ = count_error_region4(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232,\\\n",
    "                                denom, temperature)\n",
    "print err\n",
    "err, _, _ = count_error_region4(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168,\\\n",
    "                                denom, temperature)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "19.0005933588\n",
      "Wall time: 1h 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region4(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region, denom, temperature)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество немного лучшилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем признак - температура с учетом ветра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:  21888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-6.3, -9.2, -9.8, -9.8, -13.9]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"hourly_temp_wind_data.pkl\", 'rb') as fid:\n",
    "    temperature_wind = pickle.load(fid)\n",
    "print \"Data size: \", len(temperature_wind)\n",
    "temperature_wind = map(float, temperature_wind)\n",
    "temperature_wind[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region5(data, train_time_limit, test_time_limit, region, denom, temperature, temperature_wind,\\\n",
    "                        pred_start='2016-05-01', pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    #*************\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    #********\n",
    "    # Temperature\n",
    "    new_data['temperature'] = temperature[:new_data.shape[0]]\n",
    "    new_data['temperature_wind'] = temperature[:new_data.shape[0]]\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108014167034\n",
      "0.721861187392\n",
      "0.176647575811\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region5(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075,\\\n",
    "                                denom, temperature, temperature_wind)\n",
    "print err\n",
    "err, _, _ = count_error_region5(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232,\\\n",
    "                                denom, temperature, temperature_wind)\n",
    "print err\n",
    "err, _, _ = count_error_region5(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168,\\\n",
    "                                denom, temperature, temperature_wind)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18.9890127521\n",
      "Wall time: 33min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region5(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region,\\\n",
    "                                         denom, temperature, temperature_wind)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество еще улучшилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие признаки: количество осадков за день, скорость ветра, потом введем dummy-признаки погодных событий: туман, снег, дождь. (Используем для всех часов в один день одни и теже значения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Temp high</th>\n",
       "      <th>Temp avg</th>\n",
       "      <th>Temp low</th>\n",
       "      <th>Dew high</th>\n",
       "      <th>Dew avg</th>\n",
       "      <th>Dew low</th>\n",
       "      <th>Hum high</th>\n",
       "      <th>Hum avg</th>\n",
       "      <th>Hum low</th>\n",
       "      <th>...</th>\n",
       "      <th>Press avg</th>\n",
       "      <th>Press low</th>\n",
       "      <th>Vis high</th>\n",
       "      <th>Vis avg</th>\n",
       "      <th>Vis low</th>\n",
       "      <th>Wind high</th>\n",
       "      <th>Wind avg</th>\n",
       "      <th>Wind high.1</th>\n",
       "      <th>Precip</th>\n",
       "      <th>Events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>-15</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1027</td>\n",
       "      <td>1025</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-8</td>\n",
       "      <td>-5</td>\n",
       "      <td>-7</td>\n",
       "      <td>-10</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>1015</td>\n",
       "      <td>1005</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>8.38</td>\n",
       "      <td>Mist , Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-8</td>\n",
       "      <td>-10</td>\n",
       "      <td>-13</td>\n",
       "      <td>-11</td>\n",
       "      <td>-17</td>\n",
       "      <td>-22</td>\n",
       "      <td>84</td>\n",
       "      <td>63</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>1018</td>\n",
       "      <td>1004</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>7.37</td>\n",
       "      <td>Snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-2</td>\n",
       "      <td>-7</td>\n",
       "      <td>-13</td>\n",
       "      <td>-9</td>\n",
       "      <td>-16</td>\n",
       "      <td>-21</td>\n",
       "      <td>66</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>1030</td>\n",
       "      <td>1026</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>-8</td>\n",
       "      <td>92</td>\n",
       "      <td>75</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>1023</td>\n",
       "      <td>1010</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>3.56</td>\n",
       "      <td>Mist , Rain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Temp high  Temp avg  Temp low  Dew high  Dew avg  Dew low  Hum high  \\\n",
       "0    1          1        -2        -4        -7      -11      -15        59   \n",
       "1    2          1        -3        -8        -5       -7      -10        84   \n",
       "2    3         -8       -10       -13       -11      -17      -22        84   \n",
       "3    4         -2        -7       -13        -9      -16      -21        66   \n",
       "4    5          4         1        -3         3       -3       -8        92   \n",
       "\n",
       "   Hum avg  Hum low     ...      Press avg Press low Vis high Vis avg Vis low  \\\n",
       "0       49       38     ...           1027      1025       16      16      16   \n",
       "1       70       56     ...           1015      1005       16      12       1   \n",
       "2       63       42     ...           1018      1004       16       9       0   \n",
       "3       51       35     ...           1030      1026       16      16      16   \n",
       "4       75       58     ...           1023      1010       16       5       0   \n",
       "\n",
       "  Wind high Wind avg Wind high.1 Precip       Events  \n",
       "0        23        9          37   0.00           NA  \n",
       "1        34       20          45   8.38  Mist , Snow  \n",
       "2        34       17          47   7.37         Snow  \n",
       "3        14        8          32   0.00           NA  \n",
       "4        11        6          27   3.56  Mist , Rain  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather = pd.read_csv(\"./DailyWeather.csv\", sep='\\t')\n",
    "daily_weather.fillna('NA', inplace=True)\n",
    "daily_weather.replace('T', 0, inplace=True)\n",
    "daily_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 912 entries, 0 to 911\n",
      "Data columns (total 21 columns):\n",
      "Day            912 non-null int64\n",
      "Temp high      912 non-null int64\n",
      "Temp avg       912 non-null int64\n",
      "Temp low       912 non-null int64\n",
      "Dew high       912 non-null int64\n",
      "Dew avg        912 non-null int64\n",
      "Dew low        912 non-null int64\n",
      "Hum high       912 non-null int64\n",
      "Hum avg        912 non-null int64\n",
      "Hum low        912 non-null int64\n",
      "Press high     912 non-null object\n",
      "Press avg      912 non-null object\n",
      "Press low      912 non-null object\n",
      "Vis high       912 non-null object\n",
      "Vis avg        912 non-null object\n",
      "Vis low        912 non-null object\n",
      "Wind high      912 non-null object\n",
      "Wind avg       912 non-null object\n",
      "Wind high.1    912 non-null object\n",
      "Precip         912 non-null object\n",
      "Events         912 non-null object\n",
      "dtypes: int64(10), object(11)\n",
      "memory usage: 149.7+ KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Temp high</th>\n",
       "      <th>Temp avg</th>\n",
       "      <th>Temp low</th>\n",
       "      <th>Dew high</th>\n",
       "      <th>Dew avg</th>\n",
       "      <th>Dew low</th>\n",
       "      <th>Hum high</th>\n",
       "      <th>Hum avg</th>\n",
       "      <th>Hum low</th>\n",
       "      <th>...</th>\n",
       "      <th>Press avg</th>\n",
       "      <th>Press low</th>\n",
       "      <th>Vis high</th>\n",
       "      <th>Vis avg</th>\n",
       "      <th>Vis low</th>\n",
       "      <th>Wind high</th>\n",
       "      <th>Wind avg</th>\n",
       "      <th>Wind high.1</th>\n",
       "      <th>Precip</th>\n",
       "      <th>Events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>-15</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1027</td>\n",
       "      <td>1025</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>-15</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1027</td>\n",
       "      <td>1025</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>-15</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1027</td>\n",
       "      <td>1025</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>-15</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1027</td>\n",
       "      <td>1025</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-4</td>\n",
       "      <td>-7</td>\n",
       "      <td>-11</td>\n",
       "      <td>-15</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>1027</td>\n",
       "      <td>1025</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Temp high  Temp avg  Temp low  Dew high  Dew avg  Dew low  Hum high  \\\n",
       "0    1          1        -2        -4        -7      -11      -15        59   \n",
       "1    1          1        -2        -4        -7      -11      -15        59   \n",
       "2    1          1        -2        -4        -7      -11      -15        59   \n",
       "3    1          1        -2        -4        -7      -11      -15        59   \n",
       "4    1          1        -2        -4        -7      -11      -15        59   \n",
       "\n",
       "   Hum avg  Hum low  ...   Press avg Press low Vis high Vis avg Vis low  \\\n",
       "0       49       38  ...        1027      1025       16      16      16   \n",
       "1       49       38  ...        1027      1025       16      16      16   \n",
       "2       49       38  ...        1027      1025       16      16      16   \n",
       "3       49       38  ...        1027      1025       16      16      16   \n",
       "4       49       38  ...        1027      1025       16      16      16   \n",
       "\n",
       "  Wind high Wind avg Wind high.1 Precip Events  \n",
       "0        23        9          37   0.00     NA  \n",
       "1        23        9          37   0.00     NA  \n",
       "2        23        9          37   0.00     NA  \n",
       "3        23        9          37   0.00     NA  \n",
       "4        23        9          37   0.00     NA  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_h = []\n",
    "for ind in daily_weather.index:\n",
    "    for j in range(24):\n",
    "        daily_weather_h.append(daily_weather.iloc[ind].tolist())\n",
    "daily_weather_h_df = pd.DataFrame(daily_weather_h, columns = daily_weather.columns)\n",
    "daily_weather_h_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_h_df.Precip = daily_weather_h_df.Precip.apply(float)\n",
    "daily_weather_h_df['Wind avg'].replace('-', 0, inplace=True)\n",
    "daily_weather_h_df['Wind avg'] = daily_weather_h_df['Wind avg'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21888, 21)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_h_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем количество осадков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region6(data, train_time_limit, test_time_limit, region, denom, temperature, temperature_wind,\\\n",
    "                        dailyWeather, pred_start='2016-05-01', pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    #*************\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    #********\n",
    "    # Temperature\n",
    "    new_data['temperature'] = temperature[:new_data.shape[0]]\n",
    "    new_data['temperature_wind'] = temperature[:new_data.shape[0]]\n",
    "    \n",
    "    # Daily precipitation\n",
    "    new_data['precipitation'] = dailyWeather['Precip'].values[:new_data.shape[0]]\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.106966956187\n",
      "0.72018189442\n",
      "0.176846396283\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region6(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_region6(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_region6(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18.9588077377\n",
      "Wall time: 36min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region6(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region,\\\n",
    "                                         denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество еще немного улучшилось - оставляем данный признак. Добавим в рассмотрение среднюю скорость ветра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region7(data, train_time_limit, test_time_limit, region, denom, temperature, temperature_wind,\\\n",
    "                        dailyWeather, pred_start='2016-05-01', pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    #*************\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    #********\n",
    "    # Temperature\n",
    "    new_data['temperature'] = temperature[:new_data.shape[0]]\n",
    "    new_data['temperature_wind'] = temperature[:new_data.shape[0]]\n",
    "    \n",
    "    # Daily precipitation\n",
    "    new_data['precipitation'] = dailyWeather['Precip'].values[:new_data.shape[0]]\n",
    "    new_data['wind_avg'] = daily_weather_h_df['Wind avg'].values[:new_data.shape[0]]\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.106993174123\n",
      "0.719541178783\n",
      "0.176854346763\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region7(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_region7(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_region7(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18.9590542188\n",
      "Wall time: 36min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region7(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region,\\\n",
    "                                         denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметно улучшения качества не получили. Добавим dummy-признаки погодных событий: снег, дождь, туман."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_region8(data, train_time_limit, test_time_limit, region, denom, temperature, temperature_wind,\\\n",
    "                        dailyWeather, pred_start='2016-05-01', pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    #*************\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    #********\n",
    "    # Temperature\n",
    "    new_data['temperature'] = temperature[:new_data.shape[0]]\n",
    "    new_data['temperature_wind'] = temperature[:new_data.shape[0]]\n",
    "    \n",
    "    # Daily precipitation\n",
    "    new_data['precipitation'] = dailyWeather['Precip'].values[:new_data.shape[0]]\n",
    "    new_data['wind_avg'] = daily_weather_h_df['Wind avg'].values[:new_data.shape[0]]\n",
    "    new_data['mist'] = [1 if 'Mist' in string else 0 for string in dailyWeather.Events.values[:new_data.shape[0]]]\n",
    "    new_data['snow'] = [1 if 'Snow' in string else 0 for string in dailyWeather.Events.values[:new_data.shape[0]]]\n",
    "    new_data['rain'] = [1 if 'Rain' in string else 0 for string in dailyWeather.Events.values[:new_data.shape[0]]]\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        regressor = ElasticNet(alpha=0.01, l1_ratio=0.4)\n",
    "        regressor.fit(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1), \\\n",
    "                      new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        prediction = regressor.predict(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.107092081043\n",
      "0.721483936195\n",
      "0.17688974258\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_region8(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_region8(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_region8(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18.9903296916\n",
      "Wall time: 41min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    res, pred, ids = count_error_region8(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region,\\\n",
    "                                         denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество модели ухудшилось, уберем данные признаки из модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попробовать добавлять еще разных параметров в модель, но остановимся на варианте 7. Далее попробуем применить другой алгоритм обучения: XGBoostRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making regression for every region and than combining the error estimate for all regions\n",
    "def count_error_regionGDB(data, train_time_limit, test_time_limit, region, denom, temperature, temperature_wind,\\\n",
    "                        dailyWeather, pred_start='2016-05-01', pred_end='2016-05-31', degree=49, K_d=2, K_h=8):\n",
    "    new_data = pd.DataFrame(data.loc[:test_time_limit][region].values, columns=['val'], \\\n",
    "                            index = data.loc[:test_time_limit].index)\n",
    "    error = 0\n",
    "    all_ids = []\n",
    "    all_preds = []\n",
    "    offset = max(24*K_d, K_h, 12)\n",
    "    # 12-hours \n",
    "    new_data['half_day_sum'] = new_data['val'].rolling(12).sum().fillna(0)\n",
    "    # 6-hours\n",
    "    new_data['quat_day_sum'] = new_data['val'].rolling(6).sum().fillna(0)\n",
    "    # 24-hours\n",
    "    new_data['day_sum'] = new_data['val'].rolling(24).sum().fillna(0)\n",
    "    # 168-hours\n",
    "    new_data['week_sum'] = new_data['val'].rolling(168).sum().fillna(0)\n",
    "    # fourier components\n",
    "    str_var = make_fourier_regressors(new_data, degree)\n",
    "    # weekday dummy components\n",
    "    week_day_str = make_dummy_weekday(new_data)\n",
    "    # day lags\n",
    "    for day_lag in range(1, K_d):\n",
    "        new_data['day_lag_'+str(day_lag)] = [0]*offset + new_data[offset-24*day_lag:-24*day_lag]['val'].values.tolist()\n",
    "    # hour lags\n",
    "    for hour_lag in range(1, K_h):\n",
    "        new_data['hour_lag_'+str(hour_lag)] = [0]*offset + new_data[offset-hour_lag:-hour_lag]['val'].values.tolist()\n",
    "        \n",
    "    # NEW FEATURES\n",
    "    ##############\n",
    "    # Celebrations\n",
    "    #*************\n",
    "    new_data['pre_new_year'] = [1 if date.month == 12 and date.day == 31 else 0 for date in new_data.index]\n",
    "    new_data['new_year'] = [1 if date.month == 1 and date.day == 1 else 0 for date in new_data.index]\n",
    "    new_data['ind_day'] = [1 if date.month == 7 and date.day == 4 else 0 for date in new_data.index]\n",
    "    new_data['valentine_day'] = [1 if date.month == 2 and date.day == 14 else 0 for date in new_data.index]\n",
    "    new_data['inag_day'] = [1 if date.month == 1 and date.day == 20 else 0 for date in new_data.index]\n",
    "    new_data['veter_day'] = [1 if date.month == 11 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['9_11_day'] = [1 if date.month == 9 and date.day == 11 else 0 for date in new_data.index]\n",
    "    new_data['xmas_day'] = [1 if date.month == 12 and date.day == 25 else 0 for date in new_data.index]\n",
    "    new_data['lut_day'] = [1 if date.month == 1 and ((date.year == 2014 and date.day == 20) or \\\n",
    "                                                     (date.year == 2015 and date.day == 19) or\\\n",
    "                                                     (date.year == 2016 and date.day == 18))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['labor_day'] = [1 if date.month == 9 and ((date.year == 2014 and date.day == 1) or \\\n",
    "                                                     (date.year == 2015 and date.day == 7) or\\\n",
    "                                                     (date.year == 2016 and date.day == 5))\n",
    "                            else 0 for date in new_data.index]\n",
    "    new_data['columb_day'] = [1 if date.month == 10 and ((date.year == 2014 and date.day == 13) or \\\n",
    "                                                     (date.year == 2015 and date.day == 12) or\\\n",
    "                                                     (date.year == 2016 and date.day == 10))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['president_day'] = [1 if date.month == 2 and ((date.year == 2014 and date.day == 17) or \\\n",
    "                                                     (date.year == 2015 and date.day == 16) or\\\n",
    "                                                     (date.year == 2016 and date.day == 15))\n",
    "                             else 0 for date in new_data.index]\n",
    "    new_data['memm_day'] = [1 if date.month == 5 and ((date.year == 2014 and date.day == 26) or \\\n",
    "                                                     (date.year == 2015 and date.day == 25) or\\\n",
    "                                                     (date.year == 2016 and date.day == 30))\n",
    "                             else 0 for date in new_data.index]  #good feature\n",
    "    new_data['flags_day'] = [1 if date.month == 6 and date.day == 14 else 0 for date in new_data.index]\n",
    "    #********\n",
    "    # Temperature\n",
    "    new_data['temperature'] = temperature[:new_data.shape[0]]\n",
    "    new_data['temperature_wind'] = temperature[:new_data.shape[0]]\n",
    "    \n",
    "    # Daily precipitation\n",
    "    new_data['precipitation'] = dailyWeather['Precip'].values[:new_data.shape[0]]\n",
    "    new_data['wind_avg'] = daily_weather_h_df['Wind avg'].values[:new_data.shape[0]]\n",
    "        \n",
    "    ##############\n",
    "        \n",
    "    # Training and predictions\n",
    "    for data_num in range(6):\n",
    "        train_num_limit = data.loc[:train_time_limit].shape[0] - data_num\n",
    "        xgb_train = xgb.DMatrix(new_data.iloc[offset:train_num_limit].drop(['val'], axis=1),\\\n",
    "                                new_data.iloc[offset+data_num+1:train_num_limit+data_num+1].val)\n",
    "        params={\n",
    "            'max_depth': 8, \n",
    "            'eta': 0.05, \n",
    "            'colsample_bytree': 1,\n",
    "            \"min_child_weight\": 8, \n",
    "            'subsample': 0.8,\n",
    "            'gamma': 1, # l2-regularization term\n",
    "            'alpha': 0.01, # l1-regularization term\n",
    "            'objective': \"reg:linear\",\n",
    "            'eval_metric': 'mae',\n",
    "            'nthread': -1\n",
    "        }\n",
    "        evals = [(xgb_train, 'train')]\n",
    "        bst = xgb.train(params, xgb_train, num_boost_round=200, early_stopping_rounds=10, evals=evals, verbose_eval=False)\n",
    "        xgb_test = xgb.DMatrix(new_data[train_time_limit:].drop(['val'], axis=1))\n",
    "        prediction = bst.predict(xgb_test)\n",
    "        prediction = map(lambda x: max(0,x), prediction) # trips number is non-negative\n",
    "        difference = denom*np.abs(prediction - \\\n",
    "                        data.loc[pred_start+' 0'+str(0+data_num)+':00:00':pred_end+' '\\\n",
    "                               +str(18+data_num)+':00:00'][region].values)\n",
    "        error += difference.sum()\n",
    "        \n",
    "        indexes = new_data[train_time_limit:].index    \n",
    "        all_ids.append((pd.Series([str(region)]*indexes.size) + '_' + map(lambda x: x.strftime('%Y-%m-%d'), indexes) + '_' \\\n",
    "                           + map(lambda x: str(x.hour) + '_' + str(data_num+1), indexes)).values)\n",
    "        all_preds.append(prediction)\n",
    "    del new_data\n",
    "    return error, all_preds, all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0971485889424\n",
      "0.587960340265\n",
      "0.165405534963\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# testing elasting net parameter\n",
    "err, _, _ = count_error_regionGDB(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1075,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_regionGDB(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 1232,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err\n",
    "err, _, _ = count_error_regionGDB(df.loc[:'2016-05-31 23:00:00'], '2016-04-30 23:00:00', '2016-05-31 17:00:00', 2168,\\\n",
    "                                denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "print err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075 1076 1077 1125 1126 1127 1128 1129 1130 1131 1132 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1221 1222 1223 1224 1225 1227 1228 1229 1230 1231 1232 1233 1234 1235 1272 1273 1274 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1326 1327 1331 1332 1333 1334 1335 1336 1337 1338 1339 1376 1377 1378 1380 1382 1383 1384 1385 1386 1387 1388 1389 1390 1426 1431 1434 1435 1436 1437 1438 1439 1441 1442 1480 1482 1483 1530 1532 1533 1580 1630 1684 1733 1734 1783 2068 2069 2118 2119 2168 \n",
      "16.0830608085\n",
      "Wall time: 2h 29min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 739\n",
    "Q_may = 0\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_may = 0\n",
    "for region in df.columns:\n",
    "    print region,\n",
    "    res, pred, ids = count_error_regionGDB(df.loc[:'2016-05-31 23:00:00'], \\\n",
    "                                        '2016-04-30 23:00:00', '2016-05-31 17:00:00', region,\\\n",
    "                                         denom, temperature, temperature_wind, daily_weather_h_df)\n",
    "    Q_may += res\n",
    "print \"\\n\", Q_may"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество заметно улучшилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим модели на данных до мая 2016 года и посчитаем предсказания. Также еще добавим дневных лагов (это  только улучшит модель, они не были добавлены только для ускорения расчетов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1075 1076 1077 1125 1126 1127 1128 1129 1130 1131 1132 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1221 1222 1223 1224 1225 1227 1228 1229 1230 1231 1232 1233 1234 1235 1272 1273 1274 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1326 1327 1331 1332 1333 1334 1335 1336 1337 1338 1339 1376 1377 1378 1380 1382 1383 1384 1385 1386 1387 1388 1389 1390 1426 1431 1434 1435 1436 1437 1438 1439 1441 1442 1480 1482 1483 1530 1532 1533 1580 1630 1684 1733 1734 1783 2068 2069 2118 2119 2168Wall time: 2h 40min 13s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "R = 102\n",
    "H = 715\n",
    "denom = 1.0/(R*H*6)\n",
    "Q_june = 0\n",
    "all_ids = []\n",
    "all_preds = []\n",
    "for region in df.columns:\n",
    "    print region,\n",
    "    res, pred, ids = count_error_regionGDB(df.loc[:'2016-06-30 23:00:00'], \\\n",
    "                                           '2016-05-31 23:00:00', '2016-06-30 17:00:00', region,\\\n",
    "                                           denom, temperature, temperature_wind, daily_weather_h_df,\\\n",
    "                                           '2016-06-01', '2016-06-30', degree=49, K_d=7, K_h=8)\n",
    "    Q_june += res\n",
    "    all_ids.append(ids)\n",
    "    all_preds.append(pred)\n",
    "pred_df = pd.DataFrame(np.array(all_preds).ravel(), index=np.array(all_ids).ravel(), columns=['y'])\n",
    "pred_df.index.name = 'id'\n",
    "pred_df.to_csv(\"submission.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.615440313\n"
     ]
    }
   ],
   "source": [
    "print Q_june"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат улучшился приблизительно на 4.8 пункта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на submission https://inclass.kaggle.com/c/yellowtaxi/leaderboard?submissionId=4288490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приводится код для парсинга погоды с сайта https://www.wunderground.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_temperature = []\n",
    "data_temperature_wind = []\n",
    "data_wind = []\n",
    "p = pd.date_range(start='2014-01-01 00:00:00', end='2016-06-30 23:00:00', freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for date in p:\n",
    "    sleep(0.1*np.random.randint(1, 10))\n",
    "    req = requests.get('https://www.wunderground.com/history/airport/KNYC/{}/{}/{}/'\\\n",
    "          'DailyHistory.html?req_city=New+York&req_state=NY&req_statename=New+York&reqdb.'\\\n",
    "          'zip=10001&reqdb.magic=11&reqdb.wmo=99999&MR=1'.format(date.year, date.month, date.day))\n",
    "    print date,\n",
    "    counter = 0\n",
    "    prev_num = 11\n",
    "    parser = bs4.BeautifulSoup(req.text, 'lxml')\n",
    "    p1 = parser.findAll('tr', attrs={'class': \"no-metars\"})\n",
    "    for res in p1:\n",
    "        child = res.findChild('td')\n",
    "        if not '51' in child.text:\n",
    "            if not(date.year == 2016 and date.month == 5 and date.day in [13, 14, 15, 16, 18]):\n",
    "                continue\n",
    "        a = int(child.text[:child.text.find(':')])\n",
    "        if a != prev_num+1:\n",
    "            while a != prev_num+1 or (prev_num == 12 and a!=0):\n",
    "                if counter == 24:\n",
    "                    break\n",
    "                counter += 1\n",
    "                data_temperature.append(prev)\n",
    "                data_temperature_wind.append(prev2)\n",
    "                prev_num += 1\n",
    "                if prev_num == 12:\n",
    "                    prev_num = 0\n",
    "        if counter == 24:\n",
    "            break\n",
    "        prev_num = a\n",
    "        if prev_num == 12:\n",
    "            prev_num = 0\n",
    "        child = res.findChild('span', attrs={'class': 'wx-value'})\n",
    "        counter += 1\n",
    "        if child:\n",
    "            data_temperature.append(child.text)\n",
    "            prev = child.text\n",
    "        else:\n",
    "            data_temperature.append(prev)\n",
    "        child = child.findNext('span', attrs={'class': 'wx-value'})\n",
    "        if child:\n",
    "            data_temperature_wind.append(child.text)\n",
    "            prev2 = child.text\n",
    "        else:\n",
    "            data_temperature_wind.append(prev2)\n",
    "    if prev_num != 11:\n",
    "        while prev_num != 11:\n",
    "            if counter == 24:\n",
    "                break\n",
    "            counter += 1\n",
    "            data_temperature.append(prev)\n",
    "            data_temperature_wind.append(prev2)\n",
    "            prev_num+=1\n",
    "    print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"hourly_temp_data.pkl\", 'wb') as fid:\n",
    "    pickle.dump(data_temperature, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"hourly_temp_wind_data.pkl\", 'wb') as fid:\n",
    "    pickle.dump(data_temperature_wind, fid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
